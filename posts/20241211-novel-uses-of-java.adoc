= Novel Uses of Core Java for Low-Latency and High-Performance Systems

:doctype: article
:icons: font
:sectanchors:
:toc: left
:source-highlighter: highlight.js

// For developers chasing ultra-low-latency and high-throughput Java applications, standard approaches often fall short. This article reveals unconventional yet practical techniques—like capturing stack traces without exceptions, generating system-wide unique nanosecond timestamps, and creating “trivially copyable” objects—that push Core Java into new territory. Discover how to minimize garbage, gain more insightful diagnostics, and achieve near C++-like performance, all while maintaining a familiar Java ecosystem. If you’re ready to rethink old assumptions and unlock your system’s true potential, read on.

[.lead]
Standard Java libraries and idioms may only sometimes suffice in high-performance and low-latency Java systems.
This article explores unconventional yet practical techniques that push Core Java to its limits, focusing on
performance, diagnostics, and determinism. Drawing on experiences from building ultra-low-latency libraries and
infrastructure, we will highlight patterns such as capturing stack traces without exceptions, system-wide unique timestamps,
"trivially copyable" data types, zero-garbage strategies, and more. We will also discuss lessons from applying these
approaches in production environments, where nanosecond-level considerations are the norm.

This is taken from the transcript for https://www.youtube.com/watch?v=GZgZ3AyygGI[Novel Uses of Core Java for Low-Latency and High-Performance Systems] Moderated by https://www.linkedin.com/in/melissajmckay/[Melissa McKay].

== Introduction

Developers often rely on standard Java idioms—`throwable` hierarchies, `BigDecimal` for financial calculations,
thread-local resources, or off-the-shelf message queues. While straightforward, these approaches can impose unwanted
latency, garbage generation, or diagnostic blind spots. Even microseconds matter in low-latency trading, market data processing, or other
time-sensitive domains.

We aim to present "novel uses" of Core Java that remove such bottlenecks, allowing developers to produce cleaner,
more deterministic, and more insightful code. These techniques are not always common knowledge, yet they can
deliver substantial benefits in the right context.

== Capturing Stack Traces Without Exceptions

Most Java developers assume that `Throwable` subclasses like `Exception` or `Error` must represent actual errors.
However, you can extend `Throwable` to capture a stack trace at any point, even without throwing it.

[source,java]
----
public final class StackTrace extends Throwable {
    // Intentionally extends Throwable but never thrown
}
----

Why do this? Treating a stack trace as a standalone data structure lets you record where and when certain critical
events occur. For example, if a resource is closed prematurely, you can record a `StackTrace` at the closing point and
later attach it as a cause to an `IllegalStateException`, revealing not only that the resource was incorrectly reused
but exactly where it was closed. This is invaluable for diagnosing elusive bugs in long-lived systems.

This was covered recently in this article https://blog.vanillajava.blog/2024/11/exceptional-exception-stacktrace.html[Exceptional Exception StackTrace].

=== Performance Considerations and Opt-Out

Capturing a stack trace is expensive—on the order of a microsecond. Although that might sound small, it is significant
in performance-sensitive contexts. Hence, these facilities should often be toggled via configuration. In production,
you may default to no stack tracing for performance, enabling it selectively in test or diagnostic modes.
Making the trace-capturing conditional gives you powerful insight without imposing a constant performance penalty.

=== Deferred Exceptions and Resource Management

A `Throwable` created now but thrown later can act as a "deferred exception." Suppose you have a resource that must be
closed deterministically, like a memory-mapped file or a socket. If it is used after closing, the system can throw a new
exception, including a cause set to the `StackTrace` captured at close time. This helps you pinpoint the problematic
caller that closed the resource too early.

Likewise, tracking resource creation and usage patterns aids in post-mortem analysis. If an object requires explicit
cleanup was never closed and consequently leaked system resources, you can log the captured stack trace of its creation
to debug why it was not properly released.

=== Diagnosing Event Loop Pauses

In event-driven architectures, short pauses (even microseconds) can be problematic. By capturing stack traces
periodically, you can detect if an event loop lags beyond a threshold and snapshot the stack at that moment. Such
snapshots offer precision in identifying what caused the delay.

This technique can be extended to monitor multi-threaded resources. If a data structure is designed for single-threaded
use but inadvertently touched by another thread, you can store a trace of the first usage and later raise a clear
exception showing both the first and second usage points. These insights reduce guesswork and speed up debugging.

== System-Wide Unique Nanosecond Timestamps

High-frequency trading and similar domains demand unique, system-wide timestamps at nanosecond resolution. While
`System.nanoTime()` provides high-resolution time, guaranteeing uniqueness across the system is another challenge.
We can maintain a shared counter—backed by atomic operations—to ensure that each generated timestamp increases strictly. Embedding a host ID within the timestamp ensures uniqueness across multiple machines.

[source,java]
----
@Override
public long currentTimeNanos() {
    long time = provider.currentTimeNanos();
    long lastTime = bytes.readVolatileLong(LAST_TIME);
    long next = time - time % HOST_IDS + hostId;

    if (next > lastTime && bytes.compareAndSwapLong(LAST_TIME, lastTime, next)) {
        return next;
    }
    return currentTimeNanosLoop();
}
----

This approach yields a 64-bit timestamp that remains unique even if the system clock moves backwards or multiple
machines issue timestamps concurrently. Such IDs can serve as both timestamps and unique sequence identifiers,
simplifying logging, tracing, and correlation.

I discuss this further in this article https://blog.vanillajava.blog/2024/12/efficient-distributed-unique-timestamp.html[Efficient Distributed Unique Timestamp].


== Trivially Copyable Objects in Java

While Java does not natively define "trivially copyable" objects as C++ does, you can achieve something similar using
primitive fields and fixed layouts. By structuring data so that it can be copied en masse (e.g., using `Memory.copy()`
operations or off-heap buffers), you can transform objects directly into their binary representations and back again
with minimal overhead.

This is especially helpful for serialisation/deserialisation scenarios. You might store objects off-heap, represent
them in YAML for human readability, and then quickly map them back into on-heap data structures. Although Java does not
guarantee field order or packing, careful coding and consistent class layouts can produce significant performance gains.

This is covered in this post https://blog.vanillajava.blog/2024/12/trivially-copyable-objects-in-java.html[Trivially Copyable Objects in Java].

== Zero-Garbage Coding

Producing minimal or zero garbage reduces GC overhead and jitter. Consider message-processing pipelines that handle
tens of millions of events per minute. Traditional designs often generate gigabytes of short-lived objects, flushing
the CPU cache and triggering frequent garbage collections.

By reusing objects, pooling small, frequently-used strings, and leveraging off-heap memory, it is possible to reduce
garbage allocation to near-zero levels. This approach drastically cuts GC pauses and cache pressure, leading to
consistently low latencies.

For instance, you can keep a single object instance per event type and repeatedly reuse it by overwriting its fields
rather than creating a new instance each time. If you understand the lifecycle, this approach can yield
latency improvements by orders of magnitude.

== Enhanced Profiling and Safe Points

Profilers rely on "safe points" to capture stack traces. The JVM reduces safe points to optimise runtime performance,
sometimes skewing profiling results. To address this, you can add deliberate, safe points or hints that enable more
accurate profiling data. This ensures hot spots are attributed correctly, preventing misleading conclusions such as
`Integer.hashCode()` appears as a top CPU consumer when it is merely a victim of unfortunate sampling.

== Class-Based Caching and Vicarious Exceptions

For performance, decisions made per class—such as how to serialise it—should be cached. Java’s `ClassValue` provides
this mechanism, clearing the cache automatically when classes are unloaded. For cleaner code, you can implement
lambda-friendly versions of `ClassValue`.

Additionally, "vicarious exceptions" can bypass checked exception constraints. By carefully throwing
exceptions as unchecked at runtime, you avoid layering wrappers. This approach should be handled carefully and
reserved for internal code, which allows you to control both the thrower and the catcher.

== Choosing `double` Over BigDecimal

`BigDecimal` is safer for precise arithmetic but can be slow and memory-intensive. For high-performance scenarios,
`double` arithmetic is often sufficient. Although `double` is susceptible to rounding errors, those errors are
easier to spot and correct. `double`-based operations are simpler, faster, and produce no additional objects. Switching to `double` for critical performance hotspots can be worth the trade-off.

This was covered recently in this article https://blog.vanillajava.blog/2024/11/overview-many-developers-consider.html[Overview Many Developers Consider].

== Deterministic Resource Cleanup

Relying on garbage collection for resource cleanup is risky in low-latency applications. GC may run unpredictably,
leaving file handles, off-heap memory regions, or sockets dangling. Consider
cleaning resources when threads terminate or implement your lifecycle management routines to ensure deterministic cleanup.

For example, creating custom thread classes that proactively clean thread-locals upon termination ensures no
resources remain in limbo. Though admittedly hacky, this technique helps maintain deterministic behaviour in
mission-critical environments.

== Lightweight Object Pools for Strings

String interning is built into Java for compile-time constants but not for dynamic strings. Manually caching and
reusing commonly-occurring strings can reduce allocation churn. Using a small, lock-free caching array of strings,
you can often return references to previously interned strings without the overhead of global interning or heavy
hash maps.

While this technique is best for stable sets of strings, it can be combined with other no-garbage techniques to
further stabilise performance under high load.

== Summary of Key Points

- **Stack traces as data structures:** Capturing stack traces without throwing exceptions aids post-mortem debugging.
- **Unique timestamps:** System-wide unique, nanosecond-level timestamps simplify event correlation.
- **Trivially copyable objects:** Structuring data layouts and using off-heap memory can yield near-direct memory copies.
- **Zero-garbage code:** Minimising or eliminating allocation reduces GC jitter and improves predictability.
- **`double` vs BigDecimal:** For performance-critical code, `double` often outperforms `BigDecimal`.
- **Deterministic cleanup:** Do not rely solely on GC—clean resources proactively.
- **Object pools and caching:** Strategic caching of strings or objects can dramatically reduce memory pressure.
- **Profiling awareness:** Introduce safe points or hints to ensure accurate profiling.

These techniques are not always necessary for every Java application. However, in domains where latency and
determinism matter—such as financial trading, real-time analytics, or IoT streaming—they can dramatically improve
throughput, reduce jitter and enhance maintainability.

== About the author

As the CEO of https://chronicle.software/[Chronicle Software^,role=external],
https://www.linkedin.com/in/peterlawrey/[Peter Lawrey^,role=external] leads the development of cutting-edge,
low-latency solutions trusted by https://chronicle.software/8-out-of-11-investment-banks/[8 out of the top 11 global investment banks^,role=external].
With decades of experience in the financial technology sector, he specialises in delivering ultra-efficient
enabling technology which empowers businesses to handle massive volumes of data with unparalleled speed
and reliability. Peter's deep technical expertise and passion for sharing knowledge have established him
as a thought leader and mentor in the Java and FinTech communities. Follow Peter on
https://bsky.app/profile/peterlawrey.bsky.social[BlueSky^,role=external] or
https://mastodon.social/@PeterLawrey[Mastodon^,role=external].

== Conclusion

Core Java offers powerful primitives that can be employed unconventionally to achieve performance levels
often considered out of reach for managed languages. By leveraging these approaches—carefully and with proper
testing—you can build systems that run significantly faster, scale more smoothly, and give you deeper insights
into their runtime behaviours. While not every application requires such extreme measures, those that do will
find these techniques indispensable.
